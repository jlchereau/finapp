{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5408d18a",
   "metadata": {},
   "source": [
    "## Temp\n",
    "\n",
    "> Just trying stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b15ab5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import json\n",
    "import pandas as pd\n",
    "from abc import ABC, abstractmethod\n",
    "from pathlib import Path\n",
    "from pydantic import BaseModel\n",
    "from typing import Union\n",
    "import aiohttp\n",
    "import aiofiles\n",
    "import hashlib\n",
    "\n",
    "TickerResult = Union[pd.DataFrame, BaseModel]\n",
    "\n",
    "class TickerModelBase(ABC):\n",
    "    cache_dir = Path(\"cache\")  # customize if needed\n",
    "\n",
    "    def __init__(self, ticker: str, retry: int = 1, delay: float = 2.0):\n",
    "        self.ticker = ticker.upper()\n",
    "        self.retry = retry\n",
    "        self.delay = delay\n",
    "\n",
    "    @abstractmethod\n",
    "    async def fetch_raw(self) -> Union[dict, str]:\n",
    "        ...\n",
    "\n",
    "    @abstractmethod\n",
    "    def parse(self, raw: Union[dict, str]) -> TickerResult:\n",
    "        ...\n",
    "\n",
    "    def cache_path(self) -> Path:\n",
    "        key = f\"{self.__class__.__name__}_{self.ticker}\".lower()\n",
    "        digest = hashlib.md5(key.encode()).hexdigest()\n",
    "        return self.cache_dir / f\"{digest}.json\"\n",
    "\n",
    "    async def read_cache(self) -> Union[dict, str, None]:\n",
    "        path = self.cache_path()\n",
    "        if path.exists():\n",
    "            async with aiofiles.open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "                return json.loads(await f.read())\n",
    "        return None\n",
    "\n",
    "    async def write_cache(self, data: Union[dict, str]):\n",
    "        path = self.cache_path()\n",
    "        path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        async with aiofiles.open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "            await f.write(json.dumps(data))\n",
    "\n",
    "    async def fetch(self, use_cache: bool = True) -> TickerResult:\n",
    "        if use_cache:\n",
    "            cached = await self.read_cache()\n",
    "            if cached:\n",
    "                return self.parse(cached)\n",
    "\n",
    "        for attempt in range(1 + self.retry):\n",
    "            try:\n",
    "                raw = await self.fetch_raw()\n",
    "                await self.write_cache(raw)\n",
    "                return self.parse(raw)\n",
    "            except Exception as e:\n",
    "                if attempt < self.retry:\n",
    "                    await asyncio.sleep(self.delay)\n",
    "                else:\n",
    "                    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28dfa2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "\n",
    "class YahooHistoryModel(TickerModelBase):\n",
    "    async def fetch_raw(self) -> dict:\n",
    "        loop = asyncio.get_running_loop()\n",
    "        df = await loop.run_in_executor(None, lambda: yf.Ticker(self.ticker).history(period=\"1y\"))\n",
    "        return df.reset_index().to_dict(orient=\"records\")\n",
    "\n",
    "    def parse(self, raw: list[dict]) -> pd.DataFrame:\n",
    "        return pd.DataFrame(raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f9d610",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "class SomeScraperModel(TickerModelBase):\n",
    "    async def fetch_raw(self) -> str:\n",
    "        url = f\"https://example.com/{self.ticker}\"\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(url) as resp:\n",
    "                return await resp.text()\n",
    "\n",
    "    def parse(self, raw: str) -> BaseModel:\n",
    "        soup = BeautifulSoup(raw, \"html.parser\")\n",
    "        value = soup.select_one(\"span#eps\").text\n",
    "        return SomeModel(eps=float(value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb97a96d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "\n",
    "async def main():\n",
    "    model = YahooHistoryModel(\"AAPL\")\n",
    "    result = await model.fetch()\n",
    "    print(result.head() if isinstance(result, pd.DataFrame) else result)\n",
    "\n",
    "asyncio.run(main())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12c90b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Union, Optional, Dict, Any, Type\n",
    "from pathlib import Path\n",
    "from datetime import datetime, date\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel\n",
    "import json\n",
    "import requests\n",
    "from time import sleep\n",
    "import logging\n",
    "\n",
    "# Base configuration model\n",
    "class ProviderConfig(BaseModel):\n",
    "    \"\"\"Base configuration for data providers\"\"\"\n",
    "    name: str\n",
    "    base_url: str\n",
    "    timeout: int = 30\n",
    "    retry_delay: int = 3\n",
    "    cache_ttl_hours: int = 24\n",
    "\n",
    "# Abstract base class for all data providers\n",
    "class BaseDataProvider(ABC):\n",
    "    \"\"\"Abstract base class for financial data providers\"\"\"\n",
    "    \n",
    "    def __init__(self, config: ProviderConfig, cache_base_path: Path):\n",
    "        self.config = config\n",
    "        self.cache_base_path = cache_base_path\n",
    "        self.logger = logging.getLogger(f\"{self.__class__.__name__}\")\n",
    "        \n",
    "    def get_data(self, ticker: str, force_refresh: bool = False) -> Union[pd.DataFrame, BaseModel]:\n",
    "        \"\"\"\n",
    "        Main entry point to get data for a ticker\n",
    "        \n",
    "        Args:\n",
    "            ticker: Stock ticker symbol\n",
    "            force_refresh: Skip cache and fetch fresh data\n",
    "            \n",
    "        Returns:\n",
    "            Either a DataFrame or Pydantic model based on provider implementation\n",
    "        \"\"\"\n",
    "        if not force_refresh:\n",
    "            cached_data = self._load_from_cache(ticker)\n",
    "            if cached_data is not None:\n",
    "                self.logger.info(f\"Cache hit for {ticker}\")\n",
    "                return cached_data\n",
    "        \n",
    "        self.logger.info(f\"Fetching fresh data for {ticker}\")\n",
    "        data = self._fetch_and_process(ticker)\n",
    "        self._save_to_cache(ticker, data)\n",
    "        return data\n",
    "    \n",
    "    def _fetch_and_process(self, ticker: str) -> Union[pd.DataFrame, BaseModel]:\n",
    "        \"\"\"Fetch raw data and process it\"\"\"\n",
    "        raw_data = self._fetch_raw_data(ticker)\n",
    "        return self._process_raw_data(ticker, raw_data)\n",
    "    \n",
    "    def _fetch_raw_data(self, ticker: str) -> str:\n",
    "        \"\"\"Fetch raw data with retry logic\"\"\"\n",
    "        url = self._build_url(ticker)\n",
    "        \n",
    "        for attempt in range(2):  # Initial attempt + 1 retry\n",
    "            try:\n",
    "                response = requests.get(url, timeout=self.config.timeout, **self._get_request_params())\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.RequestException as e:\n",
    "                if attempt == 0:\n",
    "                    self.logger.warning(f\"Request failed for {ticker}, retrying in {self.config.retry_delay}s: {e}\")\n",
    "                    sleep(self.config.retry_delay)\n",
    "                else:\n",
    "                    self.logger.error(f\"Request failed for {ticker} after retry: {e}\")\n",
    "                    raise\n",
    "    \n",
    "    def _get_cache_path(self, ticker: str) -> Path:\n",
    "        \"\"\"Get cache file path for a ticker\"\"\"\n",
    "        today = datetime.now().strftime(\"%Y%m%d\")\n",
    "        cache_dir = self.cache_base_path / self.config.name / today\n",
    "        cache_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        extension = \"parquet\" if self._returns_dataframe() else \"json\"\n",
    "        return cache_dir / f\"{ticker.upper()}.{extension}\"\n",
    "    \n",
    "    def _load_from_cache(self, ticker: str) -> Optional[Union[pd.DataFrame, BaseModel]]:\n",
    "        \"\"\"Load data from cache if available and valid\"\"\"\n",
    "        cache_path = self._get_cache_path(ticker)\n",
    "        \n",
    "        if not cache_path.exists():\n",
    "            return None\n",
    "            \n",
    "        try:\n",
    "            if self._returns_dataframe():\n",
    "                return pd.read_parquet(cache_path)\n",
    "            else:\n",
    "                with open(cache_path, 'r') as f:\n",
    "                    data = json.load(f)\n",
    "                return self._get_pydantic_model()(**data)\n",
    "        except Exception as e:\n",
    "            self.logger.warning(f\"Failed to load cache for {ticker}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def _save_to_cache(self, ticker: str, data: Union[pd.DataFrame, BaseModel]) -> None:\n",
    "        \"\"\"Save data to cache\"\"\"\n",
    "        cache_path = self._get_cache_path(ticker)\n",
    "        \n",
    "        try:\n",
    "            if isinstance(data, pd.DataFrame):\n",
    "                data.to_parquet(cache_path)\n",
    "            else:\n",
    "                with open(cache_path, 'w') as f:\n",
    "                    json.dump(data.model_dump(), f, indent=2, default=str)\n",
    "            self.logger.info(f\"Cached data for {ticker} at {cache_path}\")\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Failed to cache data for {ticker}: {e}\")\n",
    "    \n",
    "    # Abstract methods to be implemented by subclasses\n",
    "    @abstractmethod\n",
    "    def _build_url(self, ticker: str) -> str:\n",
    "        \"\"\"Build the API URL for the ticker\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _process_raw_data(self, ticker: str, raw_data: str) -> Union[pd.DataFrame, BaseModel]:\n",
    "        \"\"\"Process raw API response into final format\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _returns_dataframe(self) -> bool:\n",
    "        \"\"\"Return True if this provider returns DataFrames, False for Pydantic models\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def _get_request_params(self) -> Dict[str, Any]:\n",
    "        \"\"\"Override to add headers, auth, etc.\"\"\"\n",
    "        return {}\n",
    "    \n",
    "    def _get_pydantic_model(self) -> Type[BaseModel]:\n",
    "        \"\"\"Override to return the Pydantic model class for non-DataFrame providers\"\"\"\n",
    "        raise NotImplementedError(\"Must implement _get_pydantic_model for non-DataFrame providers\")\n",
    "\n",
    "\n",
    "# Example implementations\n",
    "class YahooFinanceConfig(ProviderConfig):\n",
    "    \"\"\"Yahoo Finance specific configuration\"\"\"\n",
    "    name: str = \"yahoo_finance\"\n",
    "    base_url: str = \"https://query1.finance.yahoo.com/v8/finance/chart/\"\n",
    "\n",
    "class YahooFinanceHistoryProvider(BaseDataProvider):\n",
    "    \"\"\"Yahoo Finance historical data provider (returns DataFrame)\"\"\"\n",
    "    \n",
    "    def _build_url(self, ticker: str) -> str:\n",
    "        return f\"{self.config.base_url}{ticker}\"\n",
    "    \n",
    "    def _returns_dataframe(self) -> bool:\n",
    "        return True\n",
    "    \n",
    "    def _process_raw_data(self, ticker: str, raw_data: str) -> pd.DataFrame:\n",
    "        \"\"\"Process Yahoo Finance JSON response into DataFrame\"\"\"\n",
    "        import json\n",
    "        data = json.loads(raw_data)\n",
    "        \n",
    "        # Extract the chart data (simplified example)\n",
    "        result = data['chart']['result'][0]\n",
    "        timestamps = result['timestamp']\n",
    "        quotes = result['indicators']['quote'][0]\n",
    "        \n",
    "        df = pd.DataFrame({\n",
    "            'timestamp': pd.to_datetime(timestamps, unit='s'),\n",
    "            'open': quotes['open'],\n",
    "            'high': quotes['high'],\n",
    "            'low': quotes['low'],\n",
    "            'close': quotes['close'],\n",
    "            'volume': quotes['volume']\n",
    "        })\n",
    "        \n",
    "        return df.set_index('timestamp')\n",
    "\n",
    "\n",
    "# Pydantic models for structured data\n",
    "class CompanyProfile(BaseModel):\n",
    "    \"\"\"Example Pydantic model for company profile data\"\"\"\n",
    "    ticker: str\n",
    "    company_name: str\n",
    "    sector: str\n",
    "    industry: str\n",
    "    market_cap: Optional[float] = None\n",
    "    pe_ratio: Optional[float] = None\n",
    "    forward_pe: Optional[float] = None\n",
    "\n",
    "class ZacksConfig(ProviderConfig):\n",
    "    \"\"\"Zacks specific configuration\"\"\"\n",
    "    name: str = \"zacks\"\n",
    "    base_url: str = \"https://api.zacks.com/v1/\"\n",
    "    api_key: str\n",
    "\n",
    "class ZacksProfileProvider(BaseDataProvider):\n",
    "    \"\"\"Zacks company profile provider (returns Pydantic model)\"\"\"\n",
    "    \n",
    "    def _build_url(self, ticker: str) -> str:\n",
    "        return f\"{self.config.base_url}company/profile/{ticker}\"\n",
    "    \n",
    "    def _returns_dataframe(self) -> bool:\n",
    "        return False\n",
    "    \n",
    "    def _get_pydantic_model(self) -> Type[BaseModel]:\n",
    "        return CompanyProfile\n",
    "    \n",
    "    def _get_request_params(self) -> Dict[str, Any]:\n",
    "        return {\"headers\": {\"X-API-Key\": self.config.api_key}}\n",
    "    \n",
    "    def _process_raw_data(self, ticker: str, raw_data: str) -> CompanyProfile:\n",
    "        \"\"\"Process Zacks JSON response into Pydantic model\"\"\"\n",
    "        import json\n",
    "        from jmespath import search\n",
    "        \n",
    "        data = json.loads(raw_data)\n",
    "        \n",
    "        # Use jmespath to extract data\n",
    "        return CompanyProfile(\n",
    "            ticker=ticker,\n",
    "            company_name=search('company.name', data) or \"\",\n",
    "            sector=search('company.sector', data) or \"\",\n",
    "            industry=search('company.industry', data) or \"\",\n",
    "            market_cap=search('metrics.market_cap', data),\n",
    "            pe_ratio=search('metrics.pe_ratio', data),\n",
    "            forward_pe=search('metrics.forward_pe', data)\n",
    "        )\n",
    "\n",
    "\n",
    "# Example XML provider for IBKR\n",
    "class IBKRConfig(ProviderConfig):\n",
    "    \"\"\"IBKR specific configuration\"\"\"\n",
    "    name: str = \"ibkr\"\n",
    "    base_url: str = \"https://api.ibkr.com/v1/\"\n",
    "    username: str\n",
    "    password: str\n",
    "\n",
    "class IBKRDataProvider(BaseDataProvider):\n",
    "    \"\"\"IBKR XML data provider example\"\"\"\n",
    "    \n",
    "    def _build_url(self, ticker: str) -> str:\n",
    "        return f\"{self.config.base_url}market-data/{ticker}\"\n",
    "    \n",
    "    def _returns_dataframe(self) -> bool:\n",
    "        return False\n",
    "    \n",
    "    def _get_pydantic_model(self) -> Type[BaseModel]:\n",
    "        return CompanyProfile  # Simplified example\n",
    "    \n",
    "    def _process_raw_data(self, ticker: str, raw_data: str) -> CompanyProfile:\n",
    "        \"\"\"Process IBKR XML response\"\"\"\n",
    "        import xml.etree.ElementTree as ET\n",
    "        \n",
    "        root = ET.fromstring(raw_data)\n",
    "        \n",
    "        # Use xpath-like operations to extract data\n",
    "        return CompanyProfile(\n",
    "            ticker=ticker,\n",
    "            company_name=root.find('.//company/name').text if root.find('.//company/name') is not None else \"\",\n",
    "            sector=root.find('.//company/sector').text if root.find('.//company/sector') is not None else \"\",\n",
    "            industry=root.find('.//company/industry').text if root.find('.//company/industry') is not None else \"\"\n",
    "        )\n",
    "\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup logging\n",
    "    logging.basicConfig(level=logging.INFO)\n",
    "    \n",
    "    # Cache directory\n",
    "    cache_path = Path(\"./cache\")\n",
    "    \n",
    "    # Yahoo Finance example\n",
    "    yahoo_config = YahooFinanceConfig()\n",
    "    yahoo_provider = YahooFinanceHistoryProvider(yahoo_config, cache_path)\n",
    "    \n",
    "    try:\n",
    "        df = yahoo_provider.get_data(\"AAPL\")\n",
    "        print(f\"Yahoo data shape: {df.shape}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Yahoo error: {e}\")\n",
    "    \n",
    "    # Zacks example\n",
    "    zacks_config = ZacksConfig(api_key=\"your-api-key\")\n",
    "    zacks_provider = ZacksProfileProvider(zacks_config, cache_path)\n",
    "    \n",
    "    try:\n",
    "        profile = zacks_provider.get_data(\"AAPL\")\n",
    "        print(f\"Zacks profile: {profile}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Zacks error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
